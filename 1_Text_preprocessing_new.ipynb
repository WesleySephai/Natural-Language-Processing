{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/WesleySephai/Natural-Language-Processing/blob/main/1_Text_preprocessing_new.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "ff78w4STzNYd"
      },
      "outputs": [],
      "source": [
        "import nltk"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UJP3pJrBzNYd",
        "outputId": "8dbf8cec-6825-4123-855f-d02eedb56e2b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ],
      "source": [
        "nltk.download('wordnet')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "jEEvwaJgzNYe"
      },
      "outputs": [],
      "source": [
        "from nltk.stem import WordNetLemmatizer\n",
        "wnl = WordNetLemmatizer()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "nLlv7jTxzNYf",
        "outputId": "eb9edc69-7f18-4381-8b9b-c88e780b6b34"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'dog'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 4
        }
      ],
      "source": [
        "wnl.lemmatize('dogs')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "As2oSBp5zNYf",
        "outputId": "431494b9-ee96-4f63-8116-9d2b4e59efb4"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'box'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 5
        }
      ],
      "source": [
        "wnl.lemmatize('boxes')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "ZWGjGDT7zNYg",
        "outputId": "44a895fe-8df8-439a-e22a-2d6e0265fdae"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'leaf'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 6
        }
      ],
      "source": [
        "wnl.lemmatize('leaves')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "RxO1IAoQzNYg",
        "outputId": "b289c593-88ad-4008-a845-7ecff4b84474"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'book'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 7
        }
      ],
      "source": [
        "wnl.lemmatize('books')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "TLf2UG3zzNYh",
        "outputId": "527548a2-3cfa-48be-b0e0-d4bffb743cf6"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'better'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 8
        }
      ],
      "source": [
        "wnl.lemmatize('better')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "Fn92t_MJzNYh",
        "outputId": "ad2a9ed4-fc21-44a4-9c50-5bc5d7a90509"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'cry'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 9
        }
      ],
      "source": [
        "wnl.lemmatize('crying')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "eGRcxPoSzNYi",
        "outputId": "bfe8d530-46ec-41be-ece5-cf493276064a"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'jumped'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 10
        }
      ],
      "source": [
        "wnl.lemmatize('jumped')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "wNurhhZmzNYi"
      },
      "outputs": [],
      "source": [
        "from nltk.stem import PorterStemmer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "OF3bEuGhzNYi",
        "outputId": "af70bc8e-dcf6-4b29-c8f7-5ae92bc4a1a4"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'jump'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 12
        }
      ],
      "source": [
        "stemmer=PorterStemmer()\n",
        "stemmer.stem(\"jumped\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "-qDTMG6qzNYj",
        "outputId": "ca1de5d4-6acb-4435-9733-096163b78962"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'better'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 13
        }
      ],
      "source": [
        "stemmer.stem(\"better\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "1nGqx9BKzNYj"
      },
      "outputs": [],
      "source": [
        "# # from nltk.corpus import stopwords\n",
        "# import nltk\n",
        "# nltk.download('stopwords')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UGmlhG2rhtJF"
      },
      "source": [
        "### Stopwords"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "Xhy_1MzUpKW5"
      },
      "outputs": [],
      "source": [
        "import nltk\n",
        "from nltk.corpus import stopwords"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NIMjVJ9VzNYj",
        "outputId": "abaf89ef-d461-4ab0-f106-ebd302973d26"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['a', 'about', 'above', 'after', 'again', 'against', 'ain', 'all', 'am', 'an', 'and', 'any', 'are', 'aren', \"aren't\", 'as', 'at', 'be', 'because', 'been', 'before', 'being', 'below', 'between', 'both', 'but', 'by', 'can', 'couldn', \"couldn't\", 'd', 'did', 'didn', \"didn't\", 'do', 'does', 'doesn', \"doesn't\", 'doing', 'don', \"don't\", 'down', 'during', 'each', 'few', 'for', 'from', 'further', 'had', 'hadn', \"hadn't\", 'has', 'hasn', \"hasn't\", 'have', 'haven', \"haven't\", 'having', 'he', \"he'd\", \"he'll\", 'her', 'here', 'hers', 'herself', \"he's\", 'him', 'himself', 'his', 'how', 'i', \"i'd\", 'if', \"i'll\", \"i'm\", 'in', 'into', 'is', 'isn', \"isn't\", 'it', \"it'd\", \"it'll\", \"it's\", 'its', 'itself', \"i've\", 'just', 'll', 'm', 'ma', 'me', 'mightn', \"mightn't\", 'more', 'most', 'mustn', \"mustn't\", 'my', 'myself', 'needn', \"needn't\", 'no', 'nor', 'not', 'now', 'o', 'of', 'off', 'on', 'once', 'only', 'or', 'other', 'our', 'ours', 'ourselves', 'out', 'over', 'own', 're', 's', 'same', 'shan', \"shan't\", 'she', \"she'd\", \"she'll\", \"she's\", 'should', 'shouldn', \"shouldn't\", \"should've\", 'so', 'some', 'such', 't', 'than', 'that', \"that'll\", 'the', 'their', 'theirs', 'them', 'themselves', 'then', 'there', 'these', 'they', \"they'd\", \"they'll\", \"they're\", \"they've\", 'this', 'those', 'through', 'to', 'too', 'under', 'until', 'up', 've', 'very', 'was', 'wasn', \"wasn't\", 'we', \"we'd\", \"we'll\", \"we're\", 'were', 'weren', \"weren't\", \"we've\", 'what', 'when', 'where', 'which', 'while', 'who', 'whom', 'why', 'will', 'with', 'won', \"won't\", 'wouldn', \"wouldn't\", 'y', 'you', \"you'd\", \"you'll\", 'your', \"you're\", 'yours', 'yourself', 'yourselves', \"you've\"]\n",
            "198\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "nltk.download('stopwords')\n",
        "print(stopwords.words('english'))\n",
        "print(len(stopwords.words('english')))\n",
        "stopw_list = stopwords.words('english')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JDDERs2KiIgJ"
      },
      "source": [
        "### Sentence Tokenization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LiA1w7KUzNYn",
        "outputId": "15fe05fb-8118-4bdf-c9dd-d15307e534a4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Natural language processing (NLP) is an interdisciplinary subfield of computer science and linguistics.\n",
            "It is p rimarily concerned with giving computers the ability to support and manipulate speech.\n",
            "It involves processing natural language datasets, such as text corpora or speech corpora, using either rule-based or probabilistic (i.e.\n",
            "statistical and, most recently, neural network-based) machine learning approaches.\n",
            "The goal is a computer capable of \"understanding\" the contents of documents, including the contextual nuances of the language within them.\n",
            "The technology can then accurately extract  information and insights contained in the documents as well as categorize and organize the documents themselves.\n",
            "Challenges in natural language processing frequently involve speech recognition, natural-language understanding, and natural-language generation.\n",
            "History of natural language processing- Natural language processing has its roots in the 1950s.\n",
            "Already in 1950, Alan Turing published an article titled \"Computing Machinery and Intelligence\" which proposed what is now called the Turing test as a criterion of intelligence, though at the time that was not articulated as a problem separate from artificial intelligence.\n",
            "The proposed test includes a task that involves the automated interpretation and generation of natural language.\n"
          ]
        }
      ],
      "source": [
        "# Install NLTK - pip install nltk OR conda install nltk\n",
        "\n",
        "# Tokenization of paragraphs/sentences\n",
        "import nltk\n",
        "# nltk.download(\"all\")\n",
        "\n",
        "paragraph = \"\"\"Natural language processing (NLP) is an interdisciplinary subfield of computer science and linguistics. It is p rimarily concerned with giving computers the ability to support and manipulate speech. It involves processing natural language datasets, such as text corpora or speech corpora, using either rule-based or probabilistic (i.e. statistical and, most recently, neural network-based) machine learning approaches. The goal is a computer capable of \"understanding\" the contents of documents, including the contextual nuances of the language within them. The technology can then accurately extract  information and insights contained in the documents as well as categorize and organize the documents themselves.\n",
        "Challenges in natural language processing frequently involve speech recognition, natural-language understanding, and natural-language generation.\n",
        "History of natural language processing- Natural language processing has its roots in the 1950s. Already in 1950, Alan Turing published an article titled \"Computing Machinery and Intelligence\" which proposed what is now called the Turing test as a criterion of intelligence, though at the time that was not articulated as a problem separate from artificial intelligence. The proposed test includes a task that involves the automated interpretation and generation of natural language.\"\"\"\n",
        "\n",
        "# Tokenizing sentences'\n",
        "nltk.download('punkt_tab')\n",
        "sentences = nltk.sent_tokenize(paragraph)\n",
        "for sent in sentences:\n",
        "    print(sent)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ip_87x2Zh1Il"
      },
      "source": [
        "### Word Tokenization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QOXAovn0Cr3U",
        "outputId": "147b00c7-966b-4a52-ec3e-15bb3edaa035"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Natural', 'language', 'processing', '(', 'NLP', ')', 'is', 'an', 'interdisciplinary', 'subfield', 'of', 'computer', 'science', 'and', 'linguistics', '.', 'It', 'is', 'p', 'rimarily', 'concerned', 'with', 'giving', 'computers', 'the', 'ability', 'to', 'support', 'and', 'manipulate', 'speech', '.', 'It', 'involves', 'processing', 'natural', 'language', 'datasets', ',', 'such', 'as', 'text', 'corpora', 'or', 'speech', 'corpora', ',', 'using', 'either', 'rule-based', 'or', 'probabilistic', '(', 'i.e', '.', 'statistical', 'and', ',', 'most', 'recently', ',', 'neural', 'network-based', ')', 'machine', 'learning', 'approaches', '.', 'The', 'goal', 'is', 'a', 'computer', 'capable', 'of', '``', 'understanding', \"''\", 'the', 'contents', 'of', 'documents', ',', 'including', 'the', 'contextual', 'nuances', 'of', 'the', 'language', 'within', 'them', '.', 'The', 'technology', 'can', 'then', 'accurately', 'extract', 'information', 'and', 'insights', 'contained', 'in', 'the', 'documents', 'as', 'well', 'as', 'categorize', 'and', 'organize', 'the', 'documents', 'themselves', '.', 'Challenges', 'in', 'natural', 'language', 'processing', 'frequently', 'involve', 'speech', 'recognition', ',', 'natural-language', 'understanding', ',', 'and', 'natural-language', 'generation', '.', 'History', 'of', 'natural', 'language', 'processing-', 'Natural', 'language', 'processing', 'has', 'its', 'roots', 'in', 'the', '1950s', '.', 'Already', 'in', '1950', ',', 'Alan', 'Turing', 'published', 'an', 'article', 'titled', '``', 'Computing', 'Machinery', 'and', 'Intelligence', \"''\", 'which', 'proposed', 'what', 'is', 'now', 'called', 'the', 'Turing', 'test', 'as', 'a', 'criterion', 'of', 'intelligence', ',', 'though', 'at', 'the', 'time', 'that', 'was', 'not', 'articulated', 'as', 'a', 'problem', 'separate', 'from', 'artificial', 'intelligence', '.', 'The', 'proposed', 'test', 'includes', 'a', 'task', 'that', 'involves', 'the', 'automated', 'interpretation', 'and', 'generation', 'of', 'natural', 'language', '.']\n"
          ]
        }
      ],
      "source": [
        "# Tokenizing words\n",
        "words = nltk.word_tokenize(paragraph)\n",
        "print(words)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dVcBduoIBvYJ",
        "outputId": "3792c982-d7e7-4328-dc25-270bfd0ba1ce"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "a\n",
            "about\n",
            "above\n",
            "after\n",
            "again\n",
            "against\n",
            "ain\n",
            "all\n",
            "am\n",
            "an\n",
            "and\n",
            "any\n",
            "are\n",
            "aren\n",
            "aren't\n",
            "as\n",
            "at\n",
            "be\n",
            "because\n",
            "been\n",
            "before\n",
            "being\n",
            "below\n",
            "between\n",
            "both\n",
            "but\n",
            "by\n",
            "can\n",
            "couldn\n",
            "couldn't\n",
            "d\n",
            "did\n",
            "didn\n",
            "didn't\n",
            "do\n",
            "does\n",
            "doesn\n",
            "doesn't\n",
            "doing\n",
            "don\n",
            "don't\n",
            "down\n",
            "during\n",
            "each\n",
            "few\n",
            "for\n",
            "from\n",
            "further\n",
            "had\n",
            "hadn\n",
            "hadn't\n",
            "has\n",
            "hasn\n",
            "hasn't\n",
            "have\n",
            "haven\n",
            "haven't\n",
            "having\n",
            "he\n",
            "he'd\n",
            "he'll\n",
            "her\n",
            "here\n",
            "hers\n",
            "herself\n",
            "he's\n",
            "him\n",
            "himself\n",
            "his\n",
            "how\n",
            "i\n",
            "i'd\n",
            "if\n",
            "i'll\n",
            "i'm\n",
            "in\n",
            "into\n",
            "is\n",
            "isn\n",
            "isn't\n",
            "it\n",
            "it'd\n",
            "it'll\n",
            "it's\n",
            "its\n",
            "itself\n",
            "i've\n",
            "just\n",
            "ll\n",
            "m\n",
            "ma\n",
            "me\n",
            "mightn\n",
            "mightn't\n",
            "more\n",
            "most\n",
            "mustn\n",
            "mustn't\n",
            "my\n",
            "myself\n",
            "needn\n",
            "needn't\n",
            "no\n",
            "nor\n",
            "not\n",
            "now\n",
            "o\n",
            "of\n",
            "off\n",
            "on\n",
            "once\n",
            "only\n",
            "or\n",
            "other\n",
            "our\n",
            "ours\n",
            "ourselves\n",
            "out\n",
            "over\n",
            "own\n",
            "re\n",
            "s\n",
            "same\n",
            "shan\n",
            "shan't\n",
            "she\n",
            "she'd\n",
            "she'll\n",
            "she's\n",
            "should\n",
            "shouldn\n",
            "shouldn't\n",
            "should've\n",
            "so\n",
            "some\n",
            "such\n",
            "t\n",
            "than\n",
            "that\n",
            "that'll\n",
            "the\n",
            "their\n",
            "theirs\n",
            "them\n",
            "themselves\n",
            "then\n",
            "there\n",
            "these\n",
            "they\n",
            "they'd\n",
            "they'll\n",
            "they're\n",
            "they've\n",
            "this\n",
            "those\n",
            "through\n",
            "to\n",
            "too\n",
            "under\n",
            "until\n",
            "up\n",
            "ve\n",
            "very\n",
            "was\n",
            "wasn\n",
            "wasn't\n",
            "we\n",
            "we'd\n",
            "we'll\n",
            "we're\n",
            "were\n",
            "weren\n",
            "weren't\n",
            "we've\n",
            "what\n",
            "when\n",
            "where\n",
            "which\n",
            "while\n",
            "who\n",
            "whom\n",
            "why\n",
            "will\n",
            "with\n",
            "won\n",
            "won't\n",
            "wouldn\n",
            "wouldn't\n",
            "y\n",
            "you\n",
            "you'd\n",
            "you'll\n",
            "your\n",
            "you're\n",
            "yours\n",
            "yourself\n",
            "yourselves\n",
            "you've\n",
            "No. of stopwords:  198\n"
          ]
        }
      ],
      "source": [
        "for word in stopwords.words('english'):\n",
        "    print(word)\n",
        "print(\"No. of stopwords: \",len(stopwords.words('english')))\n",
        "\n",
        "# Removing the stopwords\n",
        "for i in range(len(sentences)):\n",
        "    words = nltk.word_tokenize(sentences[i])\n",
        "    words = [word for word in words if word not in stopwords.words('english')]\n",
        "    sentences[i] = ' '.join(words)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5U8ig5DgzRll",
        "outputId": "2268b202-8c0e-4a70-8dde-650d3a0ca4c1"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Natural language processing ( NLP ) interdisciplinary subfield computer science linguistics .',\n",
              " 'It p rimarily concerned giving computers ability support manipulate speech .',\n",
              " 'It involves processing natural language datasets , text corpora speech corpora , using either rule-based probabilistic ( i.e .',\n",
              " 'statistical , recently , neural network-based ) machine learning approaches .',\n",
              " 'The goal computer capable `` understanding `` contents documents , including contextual nuances language within .',\n",
              " 'The technology accurately extract information insights contained documents well categorize organize documents .',\n",
              " 'Challenges natural language processing frequently involve speech recognition , natural-language understanding , natural-language generation .',\n",
              " 'History natural language processing- Natural language processing roots 1950s .',\n",
              " 'Already 1950 , Alan Turing published article titled `` Computing Machinery Intelligence `` proposed called Turing test criterion intelligence , though time articulated problem separate artificial intelligence .',\n",
              " 'The proposed test includes task involves automated interpretation generation natural language .']"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ],
      "source": [
        "sentences"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NlSwAKocjPKw"
      },
      "source": [
        "### Stemming and Lemmatization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iTj9ZDoHB2PM",
        "outputId": "566d66be-bcb4-466a-9f38-16238f8a8ad4"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['natur languag process ( nlp ) interdisciplinari subfield comput scienc linguist .',\n",
              " 'it p rimarili concern give comput abil support manipul speech .',\n",
              " 'it involv process natur languag dataset , text corpora speech corpora , use either rule-bas probabilist ( i.e .',\n",
              " 'statist , recent , neural network-bas ) machin learn approach .',\n",
              " 'the goal comput capabl `` understand `` content document , includ contextu nuanc languag within .',\n",
              " 'the technolog accur extract inform insight contain document well categor organ document .',\n",
              " 'challeng natur languag process frequent involv speech recognit , natural-languag understand , natural-languag gener .',\n",
              " 'histori natur languag processing- natur languag process root 1950 .',\n",
              " 'alreadi 1950 , alan ture publish articl titl `` comput machineri intellig `` propos call ture test criterion intellig , though time articul problem separ artifici intellig .',\n",
              " 'the propos test includ task involv autom interpret gener natur languag .']"
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ],
      "source": [
        "stemmer = PorterStemmer()\n",
        "\n",
        "# Stemming\n",
        "for i in range(len(sentences)):\n",
        "    words = nltk.word_tokenize(sentences[i])\n",
        "    words = [stemmer.stem(word) for word in words]\n",
        "    sentences[i] = ' '.join(words)\n",
        "sentences"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "PMZZqRiqA5tD"
      },
      "outputs": [],
      "source": [
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "# Lemmatization\n",
        "for i in range(len(sentences)):\n",
        "    words = nltk.word_tokenize(sentences[i])\n",
        "    words = [lemmatizer.lemmatize(word) for word in words]\n",
        "    sentences[i] = ' '.join(words)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bxoJrfbEjtCq"
      },
      "source": [
        "### Part of Speech Tagging"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SP2SOgWkDQ6W",
        "outputId": "32ee5cf9-ad65-470a-fce6-77e354ddeaa7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package averaged_perceptron_tagger_eng to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger_eng.zip.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Natural_JJ language_NN processing_NN (_( NLP_NNP )_) is_VBZ an_DT interdisciplinary_JJ subfield_NN of_IN computer_NN science_NN and_CC linguistics_NNS ._. It_PRP is_VBZ p_JJ rimarily_RB concerned_VBN with_IN giving_VBG computers_NNS the_DT ability_NN to_TO support_VB and_CC manipulate_VB speech_NN ._. It_PRP involves_VBZ processing_VBG natural_JJ language_NN datasets_NNS ,_, such_JJ as_IN text_JJ corpora_NN or_CC speech_NN corpora_NNS ,_, using_VBG either_CC rule-based_JJ or_CC probabilistic_JJ (_( i.e_JJ ._. statistical_JJ and_CC ,_, most_RBS recently_RB ,_, neural_JJ network-based_JJ )_) machine_NN learning_VBG approaches_NNS ._. The_DT goal_NN is_VBZ a_DT computer_NN capable_NN of_IN ``_`` understanding_JJ ''_'' the_DT contents_NNS of_IN documents_NNS ,_, including_VBG the_DT contextual_JJ nuances_NNS of_IN the_DT language_NN within_IN them_PRP ._. The_DT technology_NN can_MD then_RB accurately_RB extract_JJ information_NN and_CC insights_NNS contained_VBN in_IN the_DT documents_NNS as_RB well_RB as_IN categorize_NN and_CC organize_VB the_DT documents_NNS themselves_PRP ._. Challenges_NNS in_IN natural_JJ language_NN processing_NN frequently_RB involve_VBP speech_NN recognition_NN ,_, natural-language_JJ understanding_NN ,_, and_CC natural-language_JJ generation_NN ._. History_NN of_IN natural_JJ language_NN processing-_JJ Natural_NNP language_NN processing_NN has_VBZ its_PRP$ roots_NNS in_IN the_DT 1950s_CD ._. Already_RB in_IN 1950_CD ,_, Alan_NNP Turing_NNP published_VBD an_DT article_NN titled_VBN ``_`` Computing_JJ Machinery_NN and_CC Intelligence_NNP ''_'' which_WDT proposed_VBD what_WP is_VBZ now_RB called_VBN the_DT Turing_NNP test_NN as_IN a_DT criterion_NN of_IN intelligence_NN ,_, though_RB at_IN the_DT time_NN that_WDT was_VBD not_RB articulated_VBN as_IN a_DT problem_NN separate_NN from_IN artificial_JJ intelligence_NN ._. The_DT proposed_JJ test_NN includes_VBZ a_DT task_NN that_WDT involves_VBZ the_DT automated_JJ interpretation_NN and_CC generation_NN of_IN natural_JJ language_NN ._.\n"
          ]
        }
      ],
      "source": [
        "# POS Tagging\n",
        "nltk.download('averaged_perceptron_tagger_eng')\n",
        "words = nltk.word_tokenize(paragraph)\n",
        "\n",
        "tagged_words = nltk.pos_tag(words)\n",
        "\n",
        "# Tagged word paragraph\n",
        "word_tags = []\n",
        "for tw in tagged_words:\n",
        "    word_tags.append(tw[0]+\"_\"+tw[1])\n",
        "\n",
        "tagged_paragraph = ' '.join(word_tags)\n",
        "print(tagged_paragraph)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Utp5cWCm1iav",
        "outputId": "7ce3c9fa-2919-40e2-e1eb-46636da08c9c"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Natural_JJ',\n",
              " 'language_NN',\n",
              " 'processing_NN',\n",
              " '(_(',\n",
              " 'NLP_NNP',\n",
              " ')_)',\n",
              " 'is_VBZ',\n",
              " 'an_DT',\n",
              " 'interdisciplinary_JJ',\n",
              " 'subfield_NN',\n",
              " 'of_IN',\n",
              " 'computer_NN',\n",
              " 'science_NN',\n",
              " 'and_CC',\n",
              " 'linguistics_NNS',\n",
              " '._.',\n",
              " 'It_PRP',\n",
              " 'is_VBZ',\n",
              " 'p_JJ',\n",
              " 'rimarily_RB',\n",
              " 'concerned_VBN',\n",
              " 'with_IN',\n",
              " 'giving_VBG',\n",
              " 'computers_NNS',\n",
              " 'the_DT',\n",
              " 'ability_NN',\n",
              " 'to_TO',\n",
              " 'support_VB',\n",
              " 'and_CC',\n",
              " 'manipulate_VB',\n",
              " 'speech_NN',\n",
              " '._.',\n",
              " 'It_PRP',\n",
              " 'involves_VBZ',\n",
              " 'processing_VBG',\n",
              " 'natural_JJ',\n",
              " 'language_NN',\n",
              " 'datasets_NNS',\n",
              " ',_,',\n",
              " 'such_JJ',\n",
              " 'as_IN',\n",
              " 'text_JJ',\n",
              " 'corpora_NN',\n",
              " 'or_CC',\n",
              " 'speech_NN',\n",
              " 'corpora_NNS',\n",
              " ',_,',\n",
              " 'using_VBG',\n",
              " 'either_CC',\n",
              " 'rule-based_JJ',\n",
              " 'or_CC',\n",
              " 'probabilistic_JJ',\n",
              " '(_(',\n",
              " 'i.e_JJ',\n",
              " '._.',\n",
              " 'statistical_JJ',\n",
              " 'and_CC',\n",
              " ',_,',\n",
              " 'most_RBS',\n",
              " 'recently_RB',\n",
              " ',_,',\n",
              " 'neural_JJ',\n",
              " 'network-based_JJ',\n",
              " ')_)',\n",
              " 'machine_NN',\n",
              " 'learning_VBG',\n",
              " 'approaches_NNS',\n",
              " '._.',\n",
              " 'The_DT',\n",
              " 'goal_NN',\n",
              " 'is_VBZ',\n",
              " 'a_DT',\n",
              " 'computer_NN',\n",
              " 'capable_NN',\n",
              " 'of_IN',\n",
              " '``_``',\n",
              " 'understanding_JJ',\n",
              " \"''_''\",\n",
              " 'the_DT',\n",
              " 'contents_NNS',\n",
              " 'of_IN',\n",
              " 'documents_NNS',\n",
              " ',_,',\n",
              " 'including_VBG',\n",
              " 'the_DT',\n",
              " 'contextual_JJ',\n",
              " 'nuances_NNS',\n",
              " 'of_IN',\n",
              " 'the_DT',\n",
              " 'language_NN',\n",
              " 'within_IN',\n",
              " 'them_PRP',\n",
              " '._.',\n",
              " 'The_DT',\n",
              " 'technology_NN',\n",
              " 'can_MD',\n",
              " 'then_RB',\n",
              " 'accurately_RB',\n",
              " 'extract_JJ',\n",
              " 'information_NN',\n",
              " 'and_CC',\n",
              " 'insights_NNS',\n",
              " 'contained_VBN',\n",
              " 'in_IN',\n",
              " 'the_DT',\n",
              " 'documents_NNS',\n",
              " 'as_RB',\n",
              " 'well_RB',\n",
              " 'as_IN',\n",
              " 'categorize_NN',\n",
              " 'and_CC',\n",
              " 'organize_VB',\n",
              " 'the_DT',\n",
              " 'documents_NNS',\n",
              " 'themselves_PRP',\n",
              " '._.',\n",
              " 'Challenges_NNS',\n",
              " 'in_IN',\n",
              " 'natural_JJ',\n",
              " 'language_NN',\n",
              " 'processing_NN',\n",
              " 'frequently_RB',\n",
              " 'involve_VBP',\n",
              " 'speech_NN',\n",
              " 'recognition_NN',\n",
              " ',_,',\n",
              " 'natural-language_JJ',\n",
              " 'understanding_NN',\n",
              " ',_,',\n",
              " 'and_CC',\n",
              " 'natural-language_JJ',\n",
              " 'generation_NN',\n",
              " '._.',\n",
              " 'History_NN',\n",
              " 'of_IN',\n",
              " 'natural_JJ',\n",
              " 'language_NN',\n",
              " 'processing-_JJ',\n",
              " 'Natural_NNP',\n",
              " 'language_NN',\n",
              " 'processing_NN',\n",
              " 'has_VBZ',\n",
              " 'its_PRP$',\n",
              " 'roots_NNS',\n",
              " 'in_IN',\n",
              " 'the_DT',\n",
              " '1950s_CD',\n",
              " '._.',\n",
              " 'Already_RB',\n",
              " 'in_IN',\n",
              " '1950_CD',\n",
              " ',_,',\n",
              " 'Alan_NNP',\n",
              " 'Turing_NNP',\n",
              " 'published_VBD',\n",
              " 'an_DT',\n",
              " 'article_NN',\n",
              " 'titled_VBN',\n",
              " '``_``',\n",
              " 'Computing_JJ',\n",
              " 'Machinery_NN',\n",
              " 'and_CC',\n",
              " 'Intelligence_NNP',\n",
              " \"''_''\",\n",
              " 'which_WDT',\n",
              " 'proposed_VBD',\n",
              " 'what_WP',\n",
              " 'is_VBZ',\n",
              " 'now_RB',\n",
              " 'called_VBN',\n",
              " 'the_DT',\n",
              " 'Turing_NNP',\n",
              " 'test_NN',\n",
              " 'as_IN',\n",
              " 'a_DT',\n",
              " 'criterion_NN',\n",
              " 'of_IN',\n",
              " 'intelligence_NN',\n",
              " ',_,',\n",
              " 'though_RB',\n",
              " 'at_IN',\n",
              " 'the_DT',\n",
              " 'time_NN',\n",
              " 'that_WDT',\n",
              " 'was_VBD',\n",
              " 'not_RB',\n",
              " 'articulated_VBN',\n",
              " 'as_IN',\n",
              " 'a_DT',\n",
              " 'problem_NN',\n",
              " 'separate_NN',\n",
              " 'from_IN',\n",
              " 'artificial_JJ',\n",
              " 'intelligence_NN',\n",
              " '._.',\n",
              " 'The_DT',\n",
              " 'proposed_JJ',\n",
              " 'test_NN',\n",
              " 'includes_VBZ',\n",
              " 'a_DT',\n",
              " 'task_NN',\n",
              " 'that_WDT',\n",
              " 'involves_VBZ',\n",
              " 'the_DT',\n",
              " 'automated_JJ',\n",
              " 'interpretation_NN',\n",
              " 'and_CC',\n",
              " 'generation_NN',\n",
              " 'of_IN',\n",
              " 'natural_JJ',\n",
              " 'language_NN',\n",
              " '._.']"
            ]
          },
          "metadata": {},
          "execution_count": 28
        }
      ],
      "source": [
        "word_tags"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Wbw8pv0m1HAw",
        "outputId": "1f4db411-680c-49cb-97b0-ee2c99645069"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[('Natural', 'JJ'),\n",
              " ('language', 'NN'),\n",
              " ('processing', 'NN'),\n",
              " ('(', '('),\n",
              " ('NLP', 'NNP'),\n",
              " (')', ')'),\n",
              " ('is', 'VBZ'),\n",
              " ('an', 'DT'),\n",
              " ('interdisciplinary', 'JJ'),\n",
              " ('subfield', 'NN'),\n",
              " ('of', 'IN'),\n",
              " ('computer', 'NN'),\n",
              " ('science', 'NN'),\n",
              " ('and', 'CC'),\n",
              " ('linguistics', 'NNS'),\n",
              " ('.', '.'),\n",
              " ('It', 'PRP'),\n",
              " ('is', 'VBZ'),\n",
              " ('p', 'JJ'),\n",
              " ('rimarily', 'RB'),\n",
              " ('concerned', 'VBN'),\n",
              " ('with', 'IN'),\n",
              " ('giving', 'VBG'),\n",
              " ('computers', 'NNS'),\n",
              " ('the', 'DT'),\n",
              " ('ability', 'NN'),\n",
              " ('to', 'TO'),\n",
              " ('support', 'VB'),\n",
              " ('and', 'CC'),\n",
              " ('manipulate', 'VB'),\n",
              " ('speech', 'NN'),\n",
              " ('.', '.'),\n",
              " ('It', 'PRP'),\n",
              " ('involves', 'VBZ'),\n",
              " ('processing', 'VBG'),\n",
              " ('natural', 'JJ'),\n",
              " ('language', 'NN'),\n",
              " ('datasets', 'NNS'),\n",
              " (',', ','),\n",
              " ('such', 'JJ'),\n",
              " ('as', 'IN'),\n",
              " ('text', 'JJ'),\n",
              " ('corpora', 'NN'),\n",
              " ('or', 'CC'),\n",
              " ('speech', 'NN'),\n",
              " ('corpora', 'NNS'),\n",
              " (',', ','),\n",
              " ('using', 'VBG'),\n",
              " ('either', 'CC'),\n",
              " ('rule-based', 'JJ'),\n",
              " ('or', 'CC'),\n",
              " ('probabilistic', 'JJ'),\n",
              " ('(', '('),\n",
              " ('i.e', 'JJ'),\n",
              " ('.', '.'),\n",
              " ('statistical', 'JJ'),\n",
              " ('and', 'CC'),\n",
              " (',', ','),\n",
              " ('most', 'RBS'),\n",
              " ('recently', 'RB'),\n",
              " (',', ','),\n",
              " ('neural', 'JJ'),\n",
              " ('network-based', 'JJ'),\n",
              " (')', ')'),\n",
              " ('machine', 'NN'),\n",
              " ('learning', 'VBG'),\n",
              " ('approaches', 'NNS'),\n",
              " ('.', '.'),\n",
              " ('The', 'DT'),\n",
              " ('goal', 'NN'),\n",
              " ('is', 'VBZ'),\n",
              " ('a', 'DT'),\n",
              " ('computer', 'NN'),\n",
              " ('capable', 'NN'),\n",
              " ('of', 'IN'),\n",
              " ('``', '``'),\n",
              " ('understanding', 'JJ'),\n",
              " (\"''\", \"''\"),\n",
              " ('the', 'DT'),\n",
              " ('contents', 'NNS'),\n",
              " ('of', 'IN'),\n",
              " ('documents', 'NNS'),\n",
              " (',', ','),\n",
              " ('including', 'VBG'),\n",
              " ('the', 'DT'),\n",
              " ('contextual', 'JJ'),\n",
              " ('nuances', 'NNS'),\n",
              " ('of', 'IN'),\n",
              " ('the', 'DT'),\n",
              " ('language', 'NN'),\n",
              " ('within', 'IN'),\n",
              " ('them', 'PRP'),\n",
              " ('.', '.'),\n",
              " ('The', 'DT'),\n",
              " ('technology', 'NN'),\n",
              " ('can', 'MD'),\n",
              " ('then', 'RB'),\n",
              " ('accurately', 'RB'),\n",
              " ('extract', 'JJ'),\n",
              " ('information', 'NN'),\n",
              " ('and', 'CC'),\n",
              " ('insights', 'NNS'),\n",
              " ('contained', 'VBN'),\n",
              " ('in', 'IN'),\n",
              " ('the', 'DT'),\n",
              " ('documents', 'NNS'),\n",
              " ('as', 'RB'),\n",
              " ('well', 'RB'),\n",
              " ('as', 'IN'),\n",
              " ('categorize', 'NN'),\n",
              " ('and', 'CC'),\n",
              " ('organize', 'VB'),\n",
              " ('the', 'DT'),\n",
              " ('documents', 'NNS'),\n",
              " ('themselves', 'PRP'),\n",
              " ('.', '.'),\n",
              " ('Challenges', 'NNS'),\n",
              " ('in', 'IN'),\n",
              " ('natural', 'JJ'),\n",
              " ('language', 'NN'),\n",
              " ('processing', 'NN'),\n",
              " ('frequently', 'RB'),\n",
              " ('involve', 'VBP'),\n",
              " ('speech', 'NN'),\n",
              " ('recognition', 'NN'),\n",
              " (',', ','),\n",
              " ('natural-language', 'JJ'),\n",
              " ('understanding', 'NN'),\n",
              " (',', ','),\n",
              " ('and', 'CC'),\n",
              " ('natural-language', 'JJ'),\n",
              " ('generation', 'NN'),\n",
              " ('.', '.'),\n",
              " ('History', 'NN'),\n",
              " ('of', 'IN'),\n",
              " ('natural', 'JJ'),\n",
              " ('language', 'NN'),\n",
              " ('processing-', 'JJ'),\n",
              " ('Natural', 'NNP'),\n",
              " ('language', 'NN'),\n",
              " ('processing', 'NN'),\n",
              " ('has', 'VBZ'),\n",
              " ('its', 'PRP$'),\n",
              " ('roots', 'NNS'),\n",
              " ('in', 'IN'),\n",
              " ('the', 'DT'),\n",
              " ('1950s', 'CD'),\n",
              " ('.', '.'),\n",
              " ('Already', 'RB'),\n",
              " ('in', 'IN'),\n",
              " ('1950', 'CD'),\n",
              " (',', ','),\n",
              " ('Alan', 'NNP'),\n",
              " ('Turing', 'NNP'),\n",
              " ('published', 'VBD'),\n",
              " ('an', 'DT'),\n",
              " ('article', 'NN'),\n",
              " ('titled', 'VBN'),\n",
              " ('``', '``'),\n",
              " ('Computing', 'JJ'),\n",
              " ('Machinery', 'NN'),\n",
              " ('and', 'CC'),\n",
              " ('Intelligence', 'NNP'),\n",
              " (\"''\", \"''\"),\n",
              " ('which', 'WDT'),\n",
              " ('proposed', 'VBD'),\n",
              " ('what', 'WP'),\n",
              " ('is', 'VBZ'),\n",
              " ('now', 'RB'),\n",
              " ('called', 'VBN'),\n",
              " ('the', 'DT'),\n",
              " ('Turing', 'NNP'),\n",
              " ('test', 'NN'),\n",
              " ('as', 'IN'),\n",
              " ('a', 'DT'),\n",
              " ('criterion', 'NN'),\n",
              " ('of', 'IN'),\n",
              " ('intelligence', 'NN'),\n",
              " (',', ','),\n",
              " ('though', 'RB'),\n",
              " ('at', 'IN'),\n",
              " ('the', 'DT'),\n",
              " ('time', 'NN'),\n",
              " ('that', 'WDT'),\n",
              " ('was', 'VBD'),\n",
              " ('not', 'RB'),\n",
              " ('articulated', 'VBN'),\n",
              " ('as', 'IN'),\n",
              " ('a', 'DT'),\n",
              " ('problem', 'NN'),\n",
              " ('separate', 'NN'),\n",
              " ('from', 'IN'),\n",
              " ('artificial', 'JJ'),\n",
              " ('intelligence', 'NN'),\n",
              " ('.', '.'),\n",
              " ('The', 'DT'),\n",
              " ('proposed', 'JJ'),\n",
              " ('test', 'NN'),\n",
              " ('includes', 'VBZ'),\n",
              " ('a', 'DT'),\n",
              " ('task', 'NN'),\n",
              " ('that', 'WDT'),\n",
              " ('involves', 'VBZ'),\n",
              " ('the', 'DT'),\n",
              " ('automated', 'JJ'),\n",
              " ('interpretation', 'NN'),\n",
              " ('and', 'CC'),\n",
              " ('generation', 'NN'),\n",
              " ('of', 'IN'),\n",
              " ('natural', 'JJ'),\n",
              " ('language', 'NN'),\n",
              " ('.', '.')]"
            ]
          },
          "execution_count": 44,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "tagged_words"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ntJ1xwbZj0IX"
      },
      "source": [
        "### Working with Regular Expression (re) Library for string and pattern functions\n",
        "[RegEx Library Simple functions](https://www.w3schools.com/python/python_regex.asp)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "jo9A2hOEk5v3"
      },
      "outputs": [],
      "source": [
        "import re"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uMJzzpbNF7ks",
        "outputId": "09915502-7725-4ea4-dc59-92d29b71cbbf"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "002 22ve Avengers\n"
          ]
        }
      ],
      "source": [
        "pattern1 = \"00I love Avengers\" #I love Justice League\n",
        "\n",
        "# print(re.sub(r\"Avengers\",\"Justice League\",pattern1))\n",
        "\n",
        "print(re.sub(r\"[a-z]\",\"2\",pattern1,3,flags=re.I))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "et_AXwQJIj5A",
        "outputId": "ef4139a5-2243-4efc-ea50-51d6480508d2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The original string is : Goerge is the best. 1234 Doing so for ! all good ;\n",
            "The string after punctuation filter : Goerge is the best 1234 Doing so for  all good \n"
          ]
        }
      ],
      "source": [
        "\n",
        "\n",
        "# initializing string\n",
        "test_str = \"Goerge is the best. 1234 Doing so for ! all good ;\"\n",
        "\n",
        "# printing original string\n",
        "print(\"The original string is : \" + test_str)\n",
        "\n",
        "# Removing punctuations in string\n",
        "# Using regex\n",
        "res = re.sub(r'[^\\w\\s]', '', test_str)\n",
        "\n",
        "# printing result\n",
        "print(\"The string after punctuation filter : \" + res)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rg_DydlMIlau",
        "outputId": "810e0c85-b9d6-4cb1-ad04-123e1c8de522"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Words in the string: ['Hello', 'World', '12345', 'This', 'is', 'a', 'sample', 'string', 'with', 'some', 'symbols', 'It', 'contains', 'numbers', 'like', '123', 'and', '3', '14', 'and', 'alphabets', 'like', 'abc', 'and', 'XYZ', 'There', 'are', 'also', 'newline', 'characters', 'and', 'tab', 'characters', 'It', 'may', 'contain', 'email', 'address', 'of', 'the', 'form', 'twsephai', 'gmail', 'com']\n"
          ]
        }
      ],
      "source": [
        "import re\n",
        "\n",
        "# Sample string containing various elements\n",
        "sample_string = \"\"\"\n",
        "Hello, World!\n",
        "12345 -             This is a sample string with some symbols: @#$%\n",
        "It contains numbers like 123 and 3.14, and alphabets like abc and XYZ.\n",
        "There are also newline             characters\\nand\\ttab characters.\n",
        "It may contain email address of the form twsephai@gmail.com.\n",
        "\"\"\"\n",
        "\n",
        "# Extract all words from the string\n",
        "words = re.findall(r'\\b\\w+\\b', sample_string)\n",
        "print(\"Words in the string:\", words)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aMq46B_vN9S4",
        "outputId": "5adad998-ff45-46d6-ca3a-d5b5ac57708e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Numbers in the string: ['12345', '123', '3', '14']\n"
          ]
        }
      ],
      "source": [
        "# Extract all numbers from the string\n",
        "numbers = re.findall(r'\\b\\d+\\b', sample_string)\n",
        "print(\"\\nNumbers in the string:\", numbers)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pI3Jm1Ki6dTk",
        "outputId": "e952e9b7-f912-4e18-ec66-c9bab2074483"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "String with single spaces:  Hello, World! 12345 - This is a sample string with some symbols: @#$% It contains numbers like 123 and 3.14, and alphabets like abc and XYZ. There are also newline characters and tab characters. It may contain email address of the form twsephai@gmail.com. \n"
          ]
        }
      ],
      "source": [
        "#  Replace all whitespace characters with a single space\n",
        "no_whitespace = re.sub(r'\\s+', ' ', sample_string)\n",
        "print(\"\\nString with single spaces:\", no_whitespace)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W94LL8vD6VCw",
        "outputId": "0b85ba72-7d05-499b-ebdf-87e6df803bca"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Sentences in the string: ['\\nHello, World', '\\n12345 -             This is a sample string with some symbols: @#$%\\nIt contains numbers like 123 and 3', '14, and alphabets like abc and XYZ', '\\nThere are also newline             characters\\nand\\ttab characters', '\\nIt may contain email address of the form twsephai@gmail', 'com', '\\n']\n"
          ]
        }
      ],
      "source": [
        "# Extract all sentences from the string\n",
        "sentences = re.split(r'[.!?]', sample_string)\n",
        "print(\"\\nSentences in the string:\", sentences)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1-8VC1iz6YVc",
        "outputId": "2f130058-d637-4d40-eb51-cd5d2543ca49"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "String without punctuation marks: \n",
            "Hello World\n",
            "12345              This is a sample string with some symbols \n",
            "It contains numbers like 123 and 314 and alphabets like abc and XYZ\n",
            "There are also newline             characters\n",
            "and\ttab characters\n",
            "It may contain email address of the form twsephaigmailcom\n",
            "\n"
          ]
        }
      ],
      "source": [
        "#  Remove all punctuation marks from the string\n",
        "no_punctuation = re.sub(r'[^\\w\\s]', '', sample_string)\n",
        "print(\"\\nString without punctuation marks:\", no_punctuation)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XxoxvZMS6baU",
        "outputId": "3e91be1c-bd61-4d1b-e8fa-e59b0466451a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Email addresses in the string: ['twsephai@gmail.com']\n"
          ]
        }
      ],
      "source": [
        "# Find all email addresses in the string (basic email pattern)\n",
        "email_addresses = re.findall(r'\\b[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\\.[A-Z|a-z]{2,7}\\b', sample_string)\n",
        "print(\"\\nEmail addresses in the string:\", email_addresses)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "id": "AHQD-JBC6n08"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.4"
    },
    "vscode": {
      "interpreter": {
        "hash": "bb2581d2313c8de5ff979f59992136b7cd0b4a115e62fef02542a0c9e049ace1"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}